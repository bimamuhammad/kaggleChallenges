{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input/100-bird-species/'):\n#     for filename in filenames:\n#         print(os.path.join(filename))\nfor dirname in os.listdir('/kaggle/input/100-bird-species'):\n    print(dirname)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-26T00:20:18.127236Z","iopub.execute_input":"2023-07-26T00:20:18.127588Z","iopub.status.idle":"2023-07-26T00:20:18.151889Z","shell.execute_reply.started":"2023-07-26T00:20:18.127555Z","shell.execute_reply":"2023-07-26T00:20:18.150738Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"EfficientNetB0-525-(224 X 224)- 98.97.h5\nvalid\ntest\nbirds.csv\ntrain\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input/100-bird-species/'):\n#     for filename in filenames:\n#         print(os.path.join(filename))\nfor dirname in os.listdir('/kaggle/working'):\n    print(dirname)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:27:40.020561Z","iopub.execute_input":"2023-08-10T11:27:40.021840Z","iopub.status.idle":"2023-08-10T11:27:40.030913Z","shell.execute_reply.started":"2023-08-10T11:27:40.021796Z","shell.execute_reply":"2023-08-10T11:27:40.029519Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"bird_model_pers.pt\n.virtual_documents\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Examins the data","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch dataset\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# PyTorch model\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:18:00.572353Z","iopub.execute_input":"2023-08-10T13:18:00.573289Z","iopub.status.idle":"2023-08-10T13:18:05.942068Z","shell.execute_reply.started":"2023-08-10T13:18:00.573231Z","shell.execute_reply":"2023-08-10T13:18:05.940946Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Data transform to convert data to a tensor and apply normalization\n\n# augment train and validation dataset with RandomHorizontalFlip and RandomRotation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4234, 0.4089, 0.3468], std=[0.2358, 0.2295, 0.2207])\n    ])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4234, 0.4089, 0.3468], std=[0.2358, 0.2295, 0.2207])\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:18:08.892077Z","iopub.execute_input":"2023-08-10T13:18:08.892664Z","iopub.status.idle":"2023-08-10T13:18:08.901816Z","shell.execute_reply.started":"2023-08-10T13:18:08.892628Z","shell.execute_reply":"2023-08-10T13:18:08.900772Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/100-bird-species/'","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:18:12.954245Z","iopub.execute_input":"2023-08-10T13:18:12.955395Z","iopub.status.idle":"2023-08-10T13:18:12.960736Z","shell.execute_reply.started":"2023-08-10T13:18:12.955346Z","shell.execute_reply":"2023-08-10T13:18:12.959454Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# choose the training and test datasets\ntrain_data = datasets.ImageFolder(f'{data_dir}train', transform=train_transform)\nvalidation_data = datasets.ImageFolder(f'{data_dir}valid', transform=test_transform)\ntest_data = datasets.ImageFolder(f'{data_dir}test', transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:18:26.478922Z","iopub.execute_input":"2023-08-10T13:18:26.479341Z","iopub.status.idle":"2023-08-10T13:19:11.679571Z","shell.execute_reply.started":"2023-08-10T13:18:26.479304Z","shell.execute_reply":"2023-08-10T13:19:11.678521Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Determine Image Mean","metadata":{}},{"cell_type":"code","source":"mean = 0.0\nstd = 0.0\ntotal_samples = 0\n\nfor images, _ in trainloader:\n    batch_samples = images.size(0)\n    images = images.view(batch_samples, images.size(1), -1)\n    mean += images.mean(2).sum(0)\n    std += images.std(2).sum(0)\n    total_samples += batch_samples\n\nmean /= total_samples\nstd /= total_samples\n\nprint(\"Mean:\", mean)\nprint(\"Standard Deviation:\", std)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T00:37:10.908492Z","iopub.execute_input":"2023-07-26T00:37:10.909266Z","iopub.status.idle":"2023-07-26T00:51:17.390265Z","shell.execute_reply.started":"2023-07-26T00:37:10.909222Z","shell.execute_reply":"2023-07-26T00:51:17.388947Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Mean: tensor([0.4234, 0.4089, 0.3468])\nStandard Deviation: tensor([0.2358, 0.2295, 0.2207])\n","output_type":"stream"}]},{"cell_type":"code","source":"birds=list(train_data.class_to_idx.keys())","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:11.681685Z","iopub.execute_input":"2023-08-10T13:19:11.683610Z","iopub.status.idle":"2023-08-10T13:19:11.689294Z","shell.execute_reply.started":"2023-08-10T13:19:11.683579Z","shell.execute_reply":"2023-08-10T13:19:11.688336Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"output_len= len(birds)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:11.690656Z","iopub.execute_input":"2023-08-10T13:19:11.691604Z","iopub.status.idle":"2023-08-10T13:19:11.700250Z","shell.execute_reply.started":"2023-08-10T13:19:11.691564Z","shell.execute_reply":"2023-08-10T13:19:11.699209Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\nvalidloader = torch.utils.data.DataLoader(validation_data, batch_size=32, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:11.702844Z","iopub.execute_input":"2023-08-10T13:19:11.703538Z","iopub.status.idle":"2023-08-10T13:19:11.714820Z","shell.execute_reply.started":"2023-08-10T13:19:11.703488Z","shell.execute_reply":"2023-08-10T13:19:11.713905Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimages, labels = next(iter(trainloader))\n\nfig, axes=plt.subplots(figsize=(50, 50), ncols=2)\nfor idx in np.arange(2):\n    ax=axes[idx]\n    image = images[idx].numpy().transpose((1,2,0))\n#     mean = np.array([0.485, 0.456, 0.406])\n#     std = np.array([0.229, 0.224, 0.225])\n#     image = std * image + mean\n#     image = np.clip(image, 0, 1)\n    ax.imshow(image)\n    ax.set_title(birds[labels[idx].item()])","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Design Model","metadata":{}},{"cell_type":"markdown","source":"## VGG Concept","metadata":{}},{"cell_type":"code","source":"# Copy VGG structure\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1=nn.Conv2d(3, 64, 3, 1,1 ) # OP 224\n        self.conv2=nn.Conv2d(64, 64, 3, 1,1 ) # OP 224\n        self.conv3=nn.Conv2d(64, 128, 3, 1,1 ) # OP 112\n        self.conv4=nn.Conv2d(128, 128, 3, 1,1 ) # OP 112\n        self.conv5=nn.Conv2d(128, 256, 3, 1,1 ) # OP 56\n        self.conv6=nn.Conv2d(256, 256, 3, 1,1 ) # OP 56\n#         self.conv7=nn.Conv2d(256, 512, 3, 1,1 ) # OP 28\n#         self.conv8=nn.Conv2d(512, 512, 3, 1,1 ) # OP 28\n        \n        self.pool = nn.MaxPool2d(2,2)\n        self.fc1=nn.Linear(256*28*28, 525)\n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(self.conv2(x))\n        x = self.conv3(x)\n        x = self.pool(self.conv4(x))\n        x = self.conv5(x)\n        x = self.pool(self.conv6(x))\n#         x = F.relu(self.conv7(x))\n#         x = self.pool(F.relu(self.conv8(x)))\n        \n        # flatten the tensor\n        x = x.view(x.size(0), -1)\n        \n        x=self.dropout(x)\n        x=F.relu(self.fc1(x))\n        return F.log_softmax(x, dim=1)\n        \nmodel = Net()\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-08-07T23:27:26.737193Z","iopub.execute_input":"2023-08-07T23:27:26.738198Z","iopub.status.idle":"2023-08-07T23:27:27.679441Z","shell.execute_reply.started":"2023-08-07T23:27:26.738164Z","shell.execute_reply":"2023-08-07T23:27:27.678472Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=200704, out_features=525, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1=nn.Conv2d(3, 64, 3, 1,1 ) # OP 224\n        self.conv2=nn.Conv2d(64, 128, 3, 1,1 ) # OP 224\n        self.conv3=nn.Conv2d(128, 256, 3, 1,1 ) # OP 224\n        self.conv4=nn.Conv2d(256, 512, 3, 1,1 ) # OP 224\n        self.conv5=nn.Conv2d(512, 1024, 3, 1,1 ) # OP 224\n        self.conv6=nn.Conv2d(1024, 2048, 3, 1,1 ) # OP 224\n        \n        self.pool = nn.MaxPool2d(2,2)\n        \n        self.fc1 = nn.Linear(1024,512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, output_len)\n        \n        self.bc1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc4 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc5 = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc6 = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.avg = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        \n        \n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.bc1(self.conv1(x))))\n        x = self.pool(F.relu(self.bc2(self.conv2(x))))\n        x = self.pool(F.relu(self.bc3(self.conv3(x))))\n        x = self.pool(F.relu(self.bc4(self.conv4(x))))\n        x = self.pool(F.relu(self.bc5(self.conv5(x))))\n#         x = self.pool(F.relu(self.bc6(self.conv6(x))))\n        \n        # flatten the tensor\n        x=self.avg(x)\n        x = x.view(x.size(0), -1)\n        \n        x=self.dropout(F.relu(self.fc1(x)))\n        x=self.dropout(F.relu(self.fc2(x)))\n        x=F.log_softmax(self.fc3(x), dim=1)\n        \n        return x\nmodel = Net()\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:22.465399Z","iopub.execute_input":"2023-08-10T13:19:22.465758Z","iopub.status.idle":"2023-08-10T13:19:22.745081Z","shell.execute_reply.started":"2023-08-10T13:19:22.465730Z","shell.execute_reply":"2023-08-10T13:19:22.744182Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv6): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=525, bias=True)\n  (bc1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc6): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# train_on_gpu = torch.cuda.is_available()\n# if not train_on_gpu:\n#     print('CUDA is not available.  Training on CPU ...')\n# else:\n#     model.cuda()\n#     print('CUDA is available!  Training on GPU ...')\n    \ndevice = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:28.320901Z","iopub.execute_input":"2023-08-10T13:19:28.321278Z","iopub.status.idle":"2023-08-10T13:19:31.964784Z","shell.execute_reply.started":"2023-08-10T13:19:28.321233Z","shell.execute_reply":"2023-08-10T13:19:31.963876Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv6): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=525, bias=True)\n  (bc1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc6): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-04T01:10:58.376723Z","iopub.execute_input":"2023-08-04T01:10:58.377194Z","iopub.status.idle":"2023-08-04T01:10:58.385921Z","shell.execute_reply.started":"2023-08-04T01:10:58.377152Z","shell.execute_reply":"2023-08-04T01:10:58.384605Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer=optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:36.508439Z","iopub.execute_input":"2023-08-10T13:19:36.508801Z","iopub.status.idle":"2023-08-10T13:19:36.514651Z","shell.execute_reply.started":"2023-08-10T13:19:36.508770Z","shell.execute_reply":"2023-08-10T13:19:36.513449Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"epochs=100\nvalid_loss_min = np.Inf #used to track change\n\nfor epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    model.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=model.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()\n              \n    # validation\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=model.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'bird_model_pers.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:19:40.426201Z","iopub.execute_input":"2023-08-10T13:19:40.426896Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 5.661705 \tValidation Loss: 5.120389 \t Accuracy: 0.037651\nValidation loss decreased (inf --> 5.120389).  Saving model ...\nEpoch: 2 \tTraining Loss: 5.168728 \tValidation Loss: 4.588397 \t Accuracy: 0.079819\nValidation loss decreased (5.120389 --> 4.588397).  Saving model ...\nEpoch: 3 \tTraining Loss: 4.893553 \tValidation Loss: 4.225442 \t Accuracy: 0.102786\nValidation loss decreased (4.588397 --> 4.225442).  Saving model ...\nEpoch: 4 \tTraining Loss: 4.652309 \tValidation Loss: 4.112549 \t Accuracy: 0.149473\nValidation loss decreased (4.225442 --> 4.112549).  Saving model ...\nEpoch: 5 \tTraining Loss: 4.444766 \tValidation Loss: 3.638631 \t Accuracy: 0.216114\nValidation loss decreased (4.112549 --> 3.638631).  Saving model ...\nEpoch: 6 \tTraining Loss: 4.240356 \tValidation Loss: 3.356988 \t Accuracy: 0.243976\nValidation loss decreased (3.638631 --> 3.356988).  Saving model ...\nEpoch: 7 \tTraining Loss: 4.057192 \tValidation Loss: 3.035359 \t Accuracy: 0.301581\nValidation loss decreased (3.356988 --> 3.035359).  Saving model ...\nEpoch: 8 \tTraining Loss: 3.898775 \tValidation Loss: 2.925838 \t Accuracy: 0.318524\nValidation loss decreased (3.035359 --> 2.925838).  Saving model ...\nEpoch: 9 \tTraining Loss: 3.746832 \tValidation Loss: 2.810283 \t Accuracy: 0.361069\nValidation loss decreased (2.925838 --> 2.810283).  Saving model ...\nEpoch: 10 \tTraining Loss: 3.621749 \tValidation Loss: 2.531948 \t Accuracy: 0.379142\nValidation loss decreased (2.810283 --> 2.531948).  Saving model ...\nEpoch: 11 \tTraining Loss: 3.490522 \tValidation Loss: 2.505969 \t Accuracy: 0.408509\nValidation loss decreased (2.531948 --> 2.505969).  Saving model ...\nEpoch: 12 \tTraining Loss: 3.371197 \tValidation Loss: 2.275645 \t Accuracy: 0.456702\nValidation loss decreased (2.505969 --> 2.275645).  Saving model ...\nEpoch: 13 \tTraining Loss: 3.274344 \tValidation Loss: 2.101750 \t Accuracy: 0.501130\nValidation loss decreased (2.275645 --> 2.101750).  Saving model ...\nEpoch: 14 \tTraining Loss: 3.172139 \tValidation Loss: 2.101401 \t Accuracy: 0.503389\nValidation loss decreased (2.101750 --> 2.101401).  Saving model ...\nEpoch: 15 \tTraining Loss: 3.069784 \tValidation Loss: 1.916505 \t Accuracy: 0.528991\nValidation loss decreased (2.101401 --> 1.916505).  Saving model ...\nEpoch: 16 \tTraining Loss: 2.972136 \tValidation Loss: 1.752899 \t Accuracy: 0.564383\nValidation loss decreased (1.916505 --> 1.752899).  Saving model ...\nEpoch: 17 \tTraining Loss: 2.887616 \tValidation Loss: 1.723349 \t Accuracy: 0.574925\nValidation loss decreased (1.752899 --> 1.723349).  Saving model ...\nEpoch: 18 \tTraining Loss: 2.826565 \tValidation Loss: 1.642647 \t Accuracy: 0.605798\nValidation loss decreased (1.723349 --> 1.642647).  Saving model ...\nEpoch: 19 \tTraining Loss: 2.757051 \tValidation Loss: 1.583428 \t Accuracy: 0.609187\nValidation loss decreased (1.642647 --> 1.583428).  Saving model ...\nEpoch: 20 \tTraining Loss: 2.688995 \tValidation Loss: 1.519799 \t Accuracy: 0.609187\nValidation loss decreased (1.583428 --> 1.519799).  Saving model ...\nEpoch: 21 \tTraining Loss: 2.616462 \tValidation Loss: 1.465922 \t Accuracy: 0.642696\nValidation loss decreased (1.519799 --> 1.465922).  Saving model ...\nEpoch: 22 \tTraining Loss: 2.572224 \tValidation Loss: 1.288779 \t Accuracy: 0.673569\nValidation loss decreased (1.465922 --> 1.288779).  Saving model ...\nEpoch: 23 \tTraining Loss: 2.510131 \tValidation Loss: 1.409625 \t Accuracy: 0.655120\nEpoch: 24 \tTraining Loss: 2.468520 \tValidation Loss: 1.300108 \t Accuracy: 0.674322\nEpoch: 25 \tTraining Loss: 2.415651 \tValidation Loss: 1.195656 \t Accuracy: 0.694277\nValidation loss decreased (1.288779 --> 1.195656).  Saving model ...\nEpoch: 26 \tTraining Loss: 2.363957 \tValidation Loss: 1.180624 \t Accuracy: 0.710843\nValidation loss decreased (1.195656 --> 1.180624).  Saving model ...\nEpoch: 27 \tTraining Loss: 2.330551 \tValidation Loss: 1.202969 \t Accuracy: 0.704443\nEpoch: 28 \tTraining Loss: 2.293059 \tValidation Loss: 1.077149 \t Accuracy: 0.722515\nValidation loss decreased (1.180624 --> 1.077149).  Saving model ...\nEpoch: 29 \tTraining Loss: 2.247849 \tValidation Loss: 1.139392 \t Accuracy: 0.717244\nEpoch: 30 \tTraining Loss: 2.213820 \tValidation Loss: 1.079380 \t Accuracy: 0.718373\nEpoch: 31 \tTraining Loss: 2.173500 \tValidation Loss: 1.150422 \t Accuracy: 0.706702\nEpoch: 32 \tTraining Loss: 2.144053 \tValidation Loss: 1.065148 \t Accuracy: 0.735693\nValidation loss decreased (1.077149 --> 1.065148).  Saving model ...\nEpoch: 33 \tTraining Loss: 2.123288 \tValidation Loss: 1.004458 \t Accuracy: 0.743976\nValidation loss decreased (1.065148 --> 1.004458).  Saving model ...\nEpoch: 34 \tTraining Loss: 2.088154 \tValidation Loss: 0.989479 \t Accuracy: 0.755648\nValidation loss decreased (1.004458 --> 0.989479).  Saving model ...\nEpoch: 35 \tTraining Loss: 2.046397 \tValidation Loss: 0.945708 \t Accuracy: 0.765060\nValidation loss decreased (0.989479 --> 0.945708).  Saving model ...\nEpoch: 36 \tTraining Loss: 2.035766 \tValidation Loss: 0.950419 \t Accuracy: 0.748870\nEpoch: 37 \tTraining Loss: 2.006423 \tValidation Loss: 0.907255 \t Accuracy: 0.774473\nValidation loss decreased (0.945708 --> 0.907255).  Saving model ...\nEpoch: 38 \tTraining Loss: 1.972344 \tValidation Loss: 0.863862 \t Accuracy: 0.782380\nValidation loss decreased (0.907255 --> 0.863862).  Saving model ...\nEpoch: 39 \tTraining Loss: 1.956901 \tValidation Loss: 0.886810 \t Accuracy: 0.780873\nEpoch: 40 \tTraining Loss: 1.932449 \tValidation Loss: 0.900625 \t Accuracy: 0.762425\nEpoch: 41 \tTraining Loss: 1.918876 \tValidation Loss: 0.847222 \t Accuracy: 0.792545\nValidation loss decreased (0.863862 --> 0.847222).  Saving model ...\nEpoch: 42 \tTraining Loss: 1.888541 \tValidation Loss: 0.773483 \t Accuracy: 0.800075\nValidation loss decreased (0.847222 --> 0.773483).  Saving model ...\nEpoch: 43 \tTraining Loss: 1.870202 \tValidation Loss: 0.830336 \t Accuracy: 0.788027\nEpoch: 44 \tTraining Loss: 1.852649 \tValidation Loss: 0.790946 \t Accuracy: 0.800828\nEpoch: 45 \tTraining Loss: 1.840400 \tValidation Loss: 0.840637 \t Accuracy: 0.790663\nEpoch: 46 \tTraining Loss: 1.828472 \tValidation Loss: 0.834832 \t Accuracy: 0.785015\nEpoch: 47 \tTraining Loss: 1.814975 \tValidation Loss: 0.800559 \t Accuracy: 0.791039\nEpoch: 48 \tTraining Loss: 1.799014 \tValidation Loss: 0.706767 \t Accuracy: 0.818148\nValidation loss decreased (0.773483 --> 0.706767).  Saving model ...\nEpoch: 49 \tTraining Loss: 1.783236 \tValidation Loss: 0.734630 \t Accuracy: 0.814759\nEpoch: 50 \tTraining Loss: 1.768066 \tValidation Loss: 0.811998 \t Accuracy: 0.790663\nEpoch: 51 \tTraining Loss: 1.742236 \tValidation Loss: 0.720845 \t Accuracy: 0.807605\nEpoch: 52 \tTraining Loss: 1.731064 \tValidation Loss: 0.709264 \t Accuracy: 0.808358\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    model.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=model.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()\n              \n    # validation\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=model.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'bird_model_pers.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:43:18.705319Z","iopub.execute_input":"2023-08-03T15:43:18.705850Z","iopub.status.idle":"2023-08-03T16:02:24.668830Z","shell.execute_reply.started":"2023-08-03T15:43:18.705811Z","shell.execute_reply":"2023-08-03T16:02:24.667360Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 5.088857 \tValidation Loss: 4.525621 \t Accuracy: 0.103916\nValidation loss decreased (4.660902 --> 4.525621).  Saving model ...\nEpoch: 2 \tTraining Loss: 5.055562 \tValidation Loss: 4.557027 \t Accuracy: 0.094503\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    model.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=model.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()\n              \n    # validation\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=model.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'bird_model_pers.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-08T01:20:41.849024Z","iopub.execute_input":"2023-08-08T01:20:41.850136Z","iopub.status.idle":"2023-08-08T01:55:30.800514Z","shell.execute_reply.started":"2023-08-08T01:20:41.850086Z","shell.execute_reply":"2023-08-08T01:55:30.798926Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 5.361595 \tValidation Loss: 4.802631 \t Accuracy: 0.053087\nValidation loss decreased (5.237848 --> 4.802631).  Saving model ...\nEpoch: 2 \tTraining Loss: 5.126322 \tValidation Loss: 4.738819 \t Accuracy: 0.066642\nValidation loss decreased (4.802631 --> 4.738819).  Saving model ...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images[33].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Transfer Learning","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms, models\npretrained = models.resnet50(pretrained=True)\nfor param in pretrained.parameters():\n  param.requires_grad = False\npretrained","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:45:16.601545Z","iopub.execute_input":"2023-08-01T00:45:16.601913Z","iopub.status.idle":"2023-08-01T00:45:17.076450Z","shell.execute_reply.started":"2023-08-01T00:45:16.601881Z","shell.execute_reply":"2023-08-01T00:45:17.075183Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"classifier = nn.Sequential(nn.Linear(2048,1024),\n                           nn.Dropout(p=0.2),\n                           nn.Linear(1024,output_len),\n                           nn.LogSoftmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:45:20.831227Z","iopub.execute_input":"2023-08-01T00:45:20.831583Z","iopub.status.idle":"2023-08-01T00:45:20.860730Z","shell.execute_reply.started":"2023-08-01T00:45:20.831553Z","shell.execute_reply":"2023-08-01T00:45:20.859729Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"pretrained.fc=classifier\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(pretrained.fc.parameters(), lr=0.03)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:19.414541Z","iopub.execute_input":"2023-08-01T01:41:19.414936Z","iopub.status.idle":"2023-08-01T01:41:19.421459Z","shell.execute_reply.started":"2023-08-01T01:41:19.414902Z","shell.execute_reply":"2023-08-01T01:41:19.420237Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\npretrained.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:26.476950Z","iopub.execute_input":"2023-08-01T01:41:26.477343Z","iopub.status.idle":"2023-08-01T01:41:26.492636Z","shell.execute_reply.started":"2023-08-01T01:41:26.477311Z","shell.execute_reply":"2023-08-01T01:41:26.491553Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=2048, out_features=1024, bias=True)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): Linear(in_features=1024, out_features=525, bias=True)\n    (3): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs=5\nvalid_loss_min = np.Inf #used to track change\nstep=0\nprint_every=70\n\n\nfor epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item() # *images.size(0)\n        \n        # validation\n#         if step%print_every==0:  \n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item() # *images.size(0)\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:32.146876Z","iopub.execute_input":"2023-08-01T01:41:32.147320Z","iopub.status.idle":"2023-08-01T02:28:44.333398Z","shell.execute_reply.started":"2023-08-01T01:41:32.147286Z","shell.execute_reply":"2023-08-01T02:28:44.332391Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 18001.101780 \tValidation Loss: 7261.096303 \t Accuracy: 0.697289\nValidation loss decreased (inf --> 7261.096303).  Saving model ...\nEpoch: 2 \tTraining Loss: 14211.570658 \tValidation Loss: 5679.569242 \t Accuracy: 0.725904\nValidation loss decreased (7261.096303 --> 5679.569242).  Saving model ...\nEpoch: 3 \tTraining Loss: 12308.089259 \tValidation Loss: 5660.502397 \t Accuracy: 0.703690\nValidation loss decreased (5679.569242 --> 5660.502397).  Saving model ...\nEpoch: 4 \tTraining Loss: 11111.513369 \tValidation Loss: 3939.102774 \t Accuracy: 0.737199\nValidation loss decreased (5660.502397 --> 3939.102774).  Saving model ...\nEpoch: 5 \tTraining Loss: 10117.043949 \tValidation Loss: 4510.536478 \t Accuracy: 0.710467\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()*images.size(0)\n        \n        # validation\n#         if step%print_every==0:  \n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()*images.size(0)\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:20:07.572891Z","iopub.execute_input":"2023-08-01T00:20:07.573660Z","iopub.status.idle":"2023-08-01T00:28:58.784084Z","shell.execute_reply.started":"2023-08-01T00:20:07.573621Z","shell.execute_reply":"2023-08-01T00:28:58.782656Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m pretrained\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m     11\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3183\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[1;32m   3176\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3177\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3179\u001b[0m             DecompressionBombWarning,\n\u001b[1;32m   3180\u001b[0m         )\n\u001b[0;32m-> 3183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(fp, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"epochs=30\nfor epoch in range(11, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()*images.size(0)\n        \n    # validation\n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()*images.size(0)\n        \n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n    \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n    \n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mno_grad\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'no_grad' is not defined"],"ename":"NameError","evalue":"name 'no_grad' is not defined","output_type":"error"}]}]}