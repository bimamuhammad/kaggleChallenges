{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input/100-bird-species/'):\n#     for filename in filenames:\n#         print(os.path.join(filename))\nfor dirname in os.listdir('/kaggle/input/100-bird-species'):\n    print(dirname)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-26T00:20:18.127236Z","iopub.execute_input":"2023-07-26T00:20:18.127588Z","iopub.status.idle":"2023-07-26T00:20:18.151889Z","shell.execute_reply.started":"2023-07-26T00:20:18.127555Z","shell.execute_reply":"2023-07-26T00:20:18.150738Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"EfficientNetB0-525-(224 X 224)- 98.97.h5\nvalid\ntest\nbirds.csv\ntrain\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Examins the data","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch dataset\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# PyTorch model\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:02:05.016144Z","iopub.execute_input":"2023-08-04T09:02:05.016502Z","iopub.status.idle":"2023-08-04T09:02:08.452644Z","shell.execute_reply.started":"2023-08-04T09:02:05.016470Z","shell.execute_reply":"2023-08-04T09:02:08.451611Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Data transform to convert data to a tensor and apply normalization\n\n# augment train and validation dataset with RandomHorizontalFlip and RandomRotation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4234, 0.4089, 0.3468], std=[0.2358, 0.2295, 0.2207])\n    ])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4234, 0.4089, 0.3468], std=[0.2358, 0.2295, 0.2207])\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:02:08.455112Z","iopub.execute_input":"2023-08-04T09:02:08.456103Z","iopub.status.idle":"2023-08-04T09:02:08.463916Z","shell.execute_reply.started":"2023-08-04T09:02:08.456066Z","shell.execute_reply":"2023-08-04T09:02:08.462877Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/100-bird-species/'","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:02:11.150358Z","iopub.execute_input":"2023-08-04T09:02:11.151026Z","iopub.status.idle":"2023-08-04T09:02:11.158694Z","shell.execute_reply.started":"2023-08-04T09:02:11.150985Z","shell.execute_reply":"2023-08-04T09:02:11.157642Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# choose the training and test datasets\ntrain_data = datasets.ImageFolder(f'{data_dir}train', transform=train_transform)\nvalidation_data = datasets.ImageFolder(f'{data_dir}valid', transform=test_transform)\ntest_data = datasets.ImageFolder(f'{data_dir}test', transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:02:14.093652Z","iopub.execute_input":"2023-08-04T09:02:14.094344Z","iopub.status.idle":"2023-08-04T09:02:58.225547Z","shell.execute_reply.started":"2023-08-04T09:02:14.094308Z","shell.execute_reply":"2023-08-04T09:02:58.224533Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Determine Image Mean","metadata":{}},{"cell_type":"code","source":"mean = 0.0\nstd = 0.0\ntotal_samples = 0\n\nfor images, _ in trainloader:\n    batch_samples = images.size(0)\n    images = images.view(batch_samples, images.size(1), -1)\n    mean += images.mean(2).sum(0)\n    std += images.std(2).sum(0)\n    total_samples += batch_samples\n\nmean /= total_samples\nstd /= total_samples\n\nprint(\"Mean:\", mean)\nprint(\"Standard Deviation:\", std)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T00:37:10.908492Z","iopub.execute_input":"2023-07-26T00:37:10.909266Z","iopub.status.idle":"2023-07-26T00:51:17.390265Z","shell.execute_reply.started":"2023-07-26T00:37:10.909222Z","shell.execute_reply":"2023-07-26T00:51:17.388947Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Mean: tensor([0.4234, 0.4089, 0.3468])\nStandard Deviation: tensor([0.2358, 0.2295, 0.2207])\n","output_type":"stream"}]},{"cell_type":"code","source":"birds=list(train_data.class_to_idx.keys())","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:03:05.360600Z","iopub.execute_input":"2023-08-04T09:03:05.361556Z","iopub.status.idle":"2023-08-04T09:03:05.367010Z","shell.execute_reply.started":"2023-08-04T09:03:05.361519Z","shell.execute_reply":"2023-08-04T09:03:05.365848Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"output_len= len(birds)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:03:09.228285Z","iopub.execute_input":"2023-08-04T09:03:09.228640Z","iopub.status.idle":"2023-08-04T09:03:09.233370Z","shell.execute_reply.started":"2023-08-04T09:03:09.228609Z","shell.execute_reply":"2023-08-04T09:03:09.232212Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\nvalidloader = torch.utils.data.DataLoader(validation_data, batch_size=32, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T09:03:13.958475Z","iopub.execute_input":"2023-08-04T09:03:13.958857Z","iopub.status.idle":"2023-08-04T09:03:13.965200Z","shell.execute_reply.started":"2023-08-04T09:03:13.958824Z","shell.execute_reply":"2023-08-04T09:03:13.964113Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimages, labels = next(iter(trainloader))\n\nfig, axes=plt.subplots(figsize=(50, 50), ncols=2)\nfor idx in np.arange(2):\n    ax=axes[idx]\n    image = images[idx].numpy().transpose((1,2,0))\n#     mean = np.array([0.485, 0.456, 0.406])\n#     std = np.array([0.229, 0.224, 0.225])\n#     image = std * image + mean\n#     image = np.clip(image, 0, 1)\n    ax.imshow(image)\n    ax.set_title(birds[labels[idx].item()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Design Model","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1=nn.Conv2d(3, 64, 3, 1,1 ) # OP 224\n        self.conv2=nn.Conv2d(64, 128, 3, 1,1 ) # OP 224\n        self.conv3=nn.Conv2d(128, 256, 3, 1,1 ) # OP 224\n        self.conv4=nn.Conv2d(256, 512, 3, 1,1 ) # OP 224\n        \n        self.pool = nn.MaxPool2d(2,2)\n        \n        self.fc1 = nn.Linear(512,256)\n        self.fc2 = nn.Linear(256, output_len)\n        \n        self.bc1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.bc4 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.avg = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        \n        \n        self.dropout=nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.bc1(self.conv1(x))))\n        x = self.pool(F.relu(self.bc2(self.conv2(x))))\n        x = self.pool(F.relu(self.bc3(self.conv3(x))))\n        x = self.pool(F.relu(self.bc4(self.conv4(x))))\n        \n        # flatten the tensor\n        x=self.avg(x)\n        x = x.view(x.size(0), -1)\n        \n        x=self.dropout(F.relu(self.fc1(x)))\n        x=self.fc2(x)\n        x=F.log_softmax(x, dim=1)\n        \n        return x\nmodel = Net()\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:52:53.857463Z","iopub.execute_input":"2023-08-04T14:52:53.857878Z","iopub.status.idle":"2023-08-04T14:52:53.893788Z","shell.execute_reply.started":"2023-08-04T14:52:53.857846Z","shell.execute_reply":"2023-08-04T14:52:53.892741Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=525, bias=True)\n  (bc1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# train_on_gpu = torch.cuda.is_available()\n# if not train_on_gpu:\n#     print('CUDA is not available.  Training on CPU ...')\n# else:\n#     model.cuda()\n#     print('CUDA is available!  Training on GPU ...')\n    \ndevice = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:52:58.717823Z","iopub.execute_input":"2023-08-04T14:52:58.718200Z","iopub.status.idle":"2023-08-04T14:52:58.731042Z","shell.execute_reply.started":"2023-08-04T14:52:58.718168Z","shell.execute_reply":"2023-08-04T14:52:58.730050Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=525, bias=True)\n  (bc1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bc4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-04T01:10:58.376723Z","iopub.execute_input":"2023-08-04T01:10:58.377194Z","iopub.status.idle":"2023-08-04T01:10:58.385921Z","shell.execute_reply.started":"2023-08-04T01:10:58.377152Z","shell.execute_reply":"2023-08-04T01:10:58.384605Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer=optim.Adam(model.parameters(), lr=0.003)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:53:02.777106Z","iopub.execute_input":"2023-08-04T14:53:02.777477Z","iopub.status.idle":"2023-08-04T14:53:02.783094Z","shell.execute_reply.started":"2023-08-04T14:53:02.777444Z","shell.execute_reply":"2023-08-04T14:53:02.781758Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"epochs=100\nvalid_loss_min = np.Inf #used to track change\n\nfor epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    model.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=model.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()\n              \n    # validation\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=model.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'bird_model_pers.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:53:05.029716Z","iopub.execute_input":"2023-08-04T14:53:05.030101Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 5.989330 \tValidation Loss: 5.687054 \t Accuracy: 0.011295\nValidation loss decreased (inf --> 5.687054).  Saving model ...\nEpoch: 2 \tTraining Loss: 5.796443 \tValidation Loss: 5.484194 \t Accuracy: 0.019578\nValidation loss decreased (5.687054 --> 5.484194).  Saving model ...\nEpoch: 3 \tTraining Loss: 5.644553 \tValidation Loss: 5.280692 \t Accuracy: 0.028614\nValidation loss decreased (5.484194 --> 5.280692).  Saving model ...\nEpoch: 4 \tTraining Loss: 5.388004 \tValidation Loss: 4.862541 \t Accuracy: 0.076807\nValidation loss decreased (5.280692 --> 4.862541).  Saving model ...\nEpoch: 5 \tTraining Loss: 5.209293 \tValidation Loss: 4.611605 \t Accuracy: 0.092997\nValidation loss decreased (4.862541 --> 4.611605).  Saving model ...\nEpoch: 6 \tTraining Loss: 5.064153 \tValidation Loss: 4.494500 \t Accuracy: 0.094503\nValidation loss decreased (4.611605 --> 4.494500).  Saving model ...\nEpoch: 7 \tTraining Loss: 4.934724 \tValidation Loss: 4.292379 \t Accuracy: 0.114458\nValidation loss decreased (4.494500 --> 4.292379).  Saving model ...\nEpoch: 8 \tTraining Loss: 4.830917 \tValidation Loss: 4.141972 \t Accuracy: 0.152108\nValidation loss decreased (4.292379 --> 4.141972).  Saving model ...\nEpoch: 9 \tTraining Loss: 4.757726 \tValidation Loss: 3.921127 \t Accuracy: 0.158509\nValidation loss decreased (4.141972 --> 3.921127).  Saving model ...\nEpoch: 10 \tTraining Loss: 4.693548 \tValidation Loss: 3.731813 \t Accuracy: 0.208208\nValidation loss decreased (3.921127 --> 3.731813).  Saving model ...\nEpoch: 11 \tTraining Loss: 4.641129 \tValidation Loss: 3.860671 \t Accuracy: 0.188253\nEpoch: 12 \tTraining Loss: 4.575162 \tValidation Loss: 3.629434 \t Accuracy: 0.217620\nValidation loss decreased (3.731813 --> 3.629434).  Saving model ...\nEpoch: 13 \tTraining Loss: 4.506695 \tValidation Loss: 3.536332 \t Accuracy: 0.245482\nValidation loss decreased (3.629434 --> 3.536332).  Saving model ...\nEpoch: 14 \tTraining Loss: 4.450251 \tValidation Loss: 3.422291 \t Accuracy: 0.267319\nValidation loss decreased (3.536332 --> 3.422291).  Saving model ...\nEpoch: 15 \tTraining Loss: 4.405216 \tValidation Loss: 3.381186 \t Accuracy: 0.269578\nValidation loss decreased (3.422291 --> 3.381186).  Saving model ...\nEpoch: 16 \tTraining Loss: 4.361563 \tValidation Loss: 3.252279 \t Accuracy: 0.287274\nValidation loss decreased (3.381186 --> 3.252279).  Saving model ...\nEpoch: 17 \tTraining Loss: 4.328216 \tValidation Loss: 3.184106 \t Accuracy: 0.295934\nValidation loss decreased (3.252279 --> 3.184106).  Saving model ...\nEpoch: 18 \tTraining Loss: 4.275376 \tValidation Loss: 3.083489 \t Accuracy: 0.307605\nValidation loss decreased (3.184106 --> 3.083489).  Saving model ...\nEpoch: 19 \tTraining Loss: 4.241782 \tValidation Loss: 3.124390 \t Accuracy: 0.308735\nEpoch: 20 \tTraining Loss: 4.181657 \tValidation Loss: 3.117176 \t Accuracy: 0.309111\nEpoch: 21 \tTraining Loss: 4.160187 \tValidation Loss: 3.043846 \t Accuracy: 0.323795\nValidation loss decreased (3.083489 --> 3.043846).  Saving model ...\nEpoch: 22 \tTraining Loss: 4.134055 \tValidation Loss: 2.942688 \t Accuracy: 0.356928\nValidation loss decreased (3.043846 --> 2.942688).  Saving model ...\nEpoch: 23 \tTraining Loss: 4.106315 \tValidation Loss: 3.071002 \t Accuracy: 0.330572\nEpoch: 24 \tTraining Loss: 4.090057 \tValidation Loss: 2.849359 \t Accuracy: 0.373494\nValidation loss decreased (2.942688 --> 2.849359).  Saving model ...\nEpoch: 25 \tTraining Loss: 4.055132 \tValidation Loss: 2.847729 \t Accuracy: 0.368223\nValidation loss decreased (2.849359 --> 2.847729).  Saving model ...\nEpoch: 26 \tTraining Loss: 4.040355 \tValidation Loss: 2.813970 \t Accuracy: 0.377259\nValidation loss decreased (2.847729 --> 2.813970).  Saving model ...\nEpoch: 27 \tTraining Loss: 4.013449 \tValidation Loss: 2.935921 \t Accuracy: 0.349774\nEpoch: 28 \tTraining Loss: 3.997607 \tValidation Loss: 2.719577 \t Accuracy: 0.382907\nValidation loss decreased (2.813970 --> 2.719577).  Saving model ...\nEpoch: 29 \tTraining Loss: 3.985137 \tValidation Loss: 2.827757 \t Accuracy: 0.363328\nEpoch: 30 \tTraining Loss: 3.963553 \tValidation Loss: 2.782483 \t Accuracy: 0.382907\nEpoch: 31 \tTraining Loss: 3.951192 \tValidation Loss: 2.782544 \t Accuracy: 0.381777\nEpoch: 32 \tTraining Loss: 3.926494 \tValidation Loss: 2.649760 \t Accuracy: 0.405497\nValidation loss decreased (2.719577 --> 2.649760).  Saving model ...\nEpoch: 33 \tTraining Loss: 3.907706 \tValidation Loss: 2.607231 \t Accuracy: 0.414533\nValidation loss decreased (2.649760 --> 2.607231).  Saving model ...\nEpoch: 34 \tTraining Loss: 3.909886 \tValidation Loss: 2.687016 \t Accuracy: 0.396837\nEpoch: 35 \tTraining Loss: 3.890410 \tValidation Loss: 2.572366 \t Accuracy: 0.407380\nValidation loss decreased (2.607231 --> 2.572366).  Saving model ...\nEpoch: 36 \tTraining Loss: 3.872722 \tValidation Loss: 2.590182 \t Accuracy: 0.408509\nEpoch: 37 \tTraining Loss: 3.853501 \tValidation Loss: 2.531898 \t Accuracy: 0.424699\nValidation loss decreased (2.572366 --> 2.531898).  Saving model ...\nEpoch: 38 \tTraining Loss: 3.846288 \tValidation Loss: 2.555495 \t Accuracy: 0.430723\nEpoch: 39 \tTraining Loss: 3.828033 \tValidation Loss: 2.455220 \t Accuracy: 0.427334\nValidation loss decreased (2.531898 --> 2.455220).  Saving model ...\nEpoch: 40 \tTraining Loss: 3.826133 \tValidation Loss: 2.545888 \t Accuracy: 0.429970\nEpoch: 41 \tTraining Loss: 3.803544 \tValidation Loss: 2.482171 \t Accuracy: 0.441642\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    model.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=model.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()\n              \n    # validation\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=model.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'bird_model_pers.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:43:18.705319Z","iopub.execute_input":"2023-08-03T15:43:18.705850Z","iopub.status.idle":"2023-08-03T16:02:24.668830Z","shell.execute_reply.started":"2023-08-03T15:43:18.705811Z","shell.execute_reply":"2023-08-03T16:02:24.667360Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true,"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 5.088857 \tValidation Loss: 4.525621 \t Accuracy: 0.103916\nValidation loss decreased (4.660902 --> 4.525621).  Saving model ...\nEpoch: 2 \tTraining Loss: 5.055562 \tValidation Loss: 4.557027 \t Accuracy: 0.094503\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"images.size(0)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T02:15:02.444510Z","iopub.execute_input":"2023-07-26T02:15:02.445072Z","iopub.status.idle":"2023-07-26T02:15:02.913676Z","shell.execute_reply.started":"2023-07-26T02:15:02.445032Z","shell.execute_reply":"2023-07-26T02:15:02.911199Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimages\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"],"ename":"NameError","evalue":"name 'images' is not defined","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images[33].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Transfer Learning","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, transforms, models\npretrained = models.resnet50(pretrained=True)\nfor param in pretrained.parameters():\n  param.requires_grad = False\npretrained","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:45:16.601545Z","iopub.execute_input":"2023-08-01T00:45:16.601913Z","iopub.status.idle":"2023-08-01T00:45:17.076450Z","shell.execute_reply.started":"2023-08-01T00:45:16.601881Z","shell.execute_reply":"2023-08-01T00:45:17.075183Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"classifier = nn.Sequential(nn.Linear(2048,1024),\n                           nn.Dropout(p=0.2),\n                           nn.Linear(1024,output_len),\n                           nn.LogSoftmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:45:20.831227Z","iopub.execute_input":"2023-08-01T00:45:20.831583Z","iopub.status.idle":"2023-08-01T00:45:20.860730Z","shell.execute_reply.started":"2023-08-01T00:45:20.831553Z","shell.execute_reply":"2023-08-01T00:45:20.859729Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"pretrained.fc=classifier\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(pretrained.fc.parameters(), lr=0.03)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:19.414541Z","iopub.execute_input":"2023-08-01T01:41:19.414936Z","iopub.status.idle":"2023-08-01T01:41:19.421459Z","shell.execute_reply.started":"2023-08-01T01:41:19.414902Z","shell.execute_reply":"2023-08-01T01:41:19.420237Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\npretrained.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:26.476950Z","iopub.execute_input":"2023-08-01T01:41:26.477343Z","iopub.status.idle":"2023-08-01T01:41:26.492636Z","shell.execute_reply.started":"2023-08-01T01:41:26.477311Z","shell.execute_reply":"2023-08-01T01:41:26.491553Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=2048, out_features=1024, bias=True)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): Linear(in_features=1024, out_features=525, bias=True)\n    (3): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs=5\nvalid_loss_min = np.Inf #used to track change\nstep=0\nprint_every=70\n\n\nfor epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item() # *images.size(0)\n        \n        # validation\n#         if step%print_every==0:  \n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item() # *images.size(0)\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0","metadata":{"execution":{"iopub.status.busy":"2023-08-01T01:41:32.146876Z","iopub.execute_input":"2023-08-01T01:41:32.147320Z","iopub.status.idle":"2023-08-01T02:28:44.333398Z","shell.execute_reply.started":"2023-08-01T01:41:32.147286Z","shell.execute_reply":"2023-08-01T02:28:44.332391Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch: 1 \tTraining Loss: 18001.101780 \tValidation Loss: 7261.096303 \t Accuracy: 0.697289\nValidation loss decreased (inf --> 7261.096303).  Saving model ...\nEpoch: 2 \tTraining Loss: 14211.570658 \tValidation Loss: 5679.569242 \t Accuracy: 0.725904\nValidation loss decreased (7261.096303 --> 5679.569242).  Saving model ...\nEpoch: 3 \tTraining Loss: 12308.089259 \tValidation Loss: 5660.502397 \t Accuracy: 0.703690\nValidation loss decreased (5679.569242 --> 5660.502397).  Saving model ...\nEpoch: 4 \tTraining Loss: 11111.513369 \tValidation Loss: 3939.102774 \t Accuracy: 0.737199\nValidation loss decreased (5660.502397 --> 3939.102774).  Saving model ...\nEpoch: 5 \tTraining Loss: 10117.043949 \tValidation Loss: 4510.536478 \t Accuracy: 0.710467\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()*images.size(0)\n        \n        # validation\n#         if step%print_every==0:  \n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()*images.size(0)\n\n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n\n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n\n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0","metadata":{"execution":{"iopub.status.busy":"2023-08-01T00:20:07.572891Z","iopub.execute_input":"2023-08-01T00:20:07.573660Z","iopub.status.idle":"2023-08-01T00:28:58.784084Z","shell.execute_reply.started":"2023-08-01T00:20:07.573621Z","shell.execute_reply":"2023-08-01T00:28:58.782656Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m pretrained\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m     11\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3183\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[1;32m   3176\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3177\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3179\u001b[0m             DecompressionBombWarning,\n\u001b[1;32m   3180\u001b[0m         )\n\u001b[0;32m-> 3183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(fp, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"epochs=30\nfor epoch in range(11, epochs+1):\n    \n    ################\n    ## train model\n    ###############\n    train_loss=0.0\n    valid_loss=0.0\n    accuracy=0.0\n    pretrained.train()\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logps=pretrained.forward(images)\n        loss=criterion(logps, labels)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss+=loss.item()*images.size(0)\n        \n    # validation\n    pretrained.eval()\n    with torch.no_grad():\n        for images, labels in validloader:\n            images, labels = images.to(device), labels.to(device)\n\n            logps=pretrained.forward(images)\n            loss=criterion(logps, labels)\n            ps=torch.exp(logps)\n            top_k, top_class = ps.topk(1, dim=1)\n            equality= top_class==labels.view(*top_class.shape)\n            accuracy+=torch.mean(equality.type(torch.FloatTensor)).item()\n\n            valid_loss+=loss.item()*images.size(0)\n        \n    train_loss=train_loss/len(trainloader)\n    valid_loss=valid_loss/len(validloader)\n    accuracy=accuracy/len(validloader)\n    \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}'.format(\n        epoch, train_loss, valid_loss, accuracy))\n    \n    # save model if validation loss has decreased\n    if valid_loss < valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(pretrained.state_dict(), 'bird_model.pt')\n        valid_loss_min = valid_loss","metadata":{"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mno_grad\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'no_grad' is not defined"],"ename":"NameError","evalue":"name 'no_grad' is not defined","output_type":"error"}]}]}